{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Travelling Salesperson Problem\n",
    "---\n",
    "\n",
    "A classic problem in operations research. See eg Norvig, http://nbviewer.ipython.org/url/norvig.com/ipython/TSP.ipynb, for a great treatment.\n",
    "\n",
    "We will represent a salesperson's tour as a list of integers, each integer an index into a list of cities. Of course, the integers must form a permutation.\n",
    "\n",
    "First, we define some generators for comparison. We will demonstrate two new ideas:\n",
    "\n",
    "1. Generators which call subroutines.\n",
    "2. Using the problem instance in a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators which call subroutines\n",
    "\n",
    "As before, we will import a `random` object in which random calls are instrumented for tracing. This time, our generators will call some subroutines which themselves make random calls. We have to inform PTO that these subroutines should be recorded in the program trace. We will do so by importing a `random_function` decorator, and applying it to each of the relevant subroutines.\n",
    "\n",
    "(In the simple cases *onemax* and *sphere*, the generator itself made random calls, but there were no subroutines. PTO knows that the generator itself should always be recorded, so in those simple cases, we didn't need to use the `random_function` decorator. By the way, if we decorate a subroutine that doesn't need to be recorded, nothing bad happens. But it would be confusing for other readers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the problem instance in a generator\n",
    "\n",
    "In `sphere`, we saw that some problems can be defined parametrically, with a *problem size* parameter, and when we want to study scalability across problem sizes, PTO provides a way to do that.\n",
    "\n",
    "Next, we'll observe that some problems are naturally parametrized in terms of a *problem instance*. The problem instance is composed of any problem-specific data, which may include problem *size* (thus instance is a generalisation of size). In PTO, it is natural for generators to look at the problem instance to influence how the make their random decisions. So, we will write some generators which take the problem instance as an argument. The problem instance will be a square city-city distance matrix named `inst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PTO import random, random_function, solve\n",
    "\n",
    "# no need for @random_function because this will be used as a generator itself\n",
    "def randsol1(inst):\n",
    "    # Create a permutation by shuffling. We provide a custom shuffle function.\n",
    "    return swap_shuffle(range(len(inst)))\n",
    "\n",
    "@random_function # inform PTO that this function must be traced\n",
    "def swap_shuffle(perm): \n",
    "    for i in range(len(perm)):\n",
    "        ri = random.choice(range(i,len(perm)))\n",
    "        perm[i],perm[ri]=perm[ri],perm[i]\n",
    "    return perm\n",
    "\n",
    "# no need for @random_function because this will be used as a generator itself\n",
    "def randsol2(inst):\n",
    "    # Create a permutation by shuffling. We provide a custom shuffle function.\n",
    "    return rev_shuffle(range(len(inst)))\n",
    "\n",
    "@random_function # inform PTO that this function must be traced\n",
    "def rev_shuffle(perm):\n",
    "    # this is like multiple applications of 2-opt\n",
    "    for i in range(len(perm)):\n",
    "        ri = random.choice(range(i,len(perm)))\n",
    "        perm[i:ri+1] = perm[i:ri+1][::-1] # reverse a section\n",
    "    return perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Really* using the problem instance: GRASP\n",
    "\n",
    "The two generators above look at the problem instance, but don't use it in any meaningful way. The next generator does: it builds up a permutation step by step, at each step choosing a city which is near the current one, with high probability. This is the essence of *GRASP*, a greedy randomized adaptive search procedure (Resende and Ribeiro, 2009). But this is still non-deterministic, so it is still suitable as a PTO generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randsol3(inst):\n",
    "    \"\"\"Take advantage of problem data in the simplest possible way\"\"\"\n",
    "    n = len(inst)\n",
    "    sol = []\n",
    "    remaining = list(range(n))\n",
    "\n",
    "    # the start city is a decision variable, because we'll get different results if\n",
    "    # we start at different cities\n",
    "\n",
    "    x = random.choice(list(range(n))) #(remaining) THIS WAS THE ROOT OF THE ERROR. \n",
    "    # Partial + Mutable in Local + argument of wrapped function = weird side effect... just avoid this combination!\n",
    "\n",
    "    sol.append(x)\n",
    "    remaining.remove(x)\n",
    "\n",
    "    i = 1\n",
    "    while i < n:\n",
    "\n",
    "        # choose one of the remaining cities randomly, weighted by\n",
    "        # inverse distance.\n",
    "        x = choose_node(inst, x, remaining)\n",
    "        sol.append(x)\n",
    "        remaining.remove(x)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return sol\n",
    "\n",
    "# no need for @random_function, because choose_node makes no random calls itself\n",
    "def choose_node(inst, cur, remaining):\n",
    "    wts = [1.0 / inst[cur][n] for n in remaining]\n",
    "    s = sum(wts)\n",
    "    wts = [wt / float(s) for wt in wts] # normalise\n",
    "    return roulette_wheel(remaining, wts)\n",
    "\n",
    "@random_function # inform PTO that this function must be traced\n",
    "def roulette_wheel(items, wts): # assumes wts sum to 1\n",
    "    x = random.random()\n",
    "    for item, wt in zip(items, wts):\n",
    "        x -= wt\n",
    "        if x <= 0:\n",
    "            return item\n",
    "    # Should not reach here\n",
    "    print(\"Error\")\n",
    "    print(items)\n",
    "    print(wts)\n",
    "    print(r)\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness\n",
    "\n",
    "Our generators take an `inst` (instance) argument. Our fitness function will too. Fitness is the negative of tour length, given a permutation `perm` and a city-city distance matrix `inst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitness(perm, inst):\n",
    "    # note negative indexing trick to include final step (return to origin)\n",
    "    return -sum([inst[perm[i-1]][perm[i]] for i in range(0,len(perm))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random problem instances\n",
    "\n",
    "We will generate random instances, given a problem size (number of cities) `n`, by generating a distance matrix with values exponentially distributed.\n",
    "\n",
    "(Note also that the use of `random` here will not be recorded by the tracer (see *Generators that call subroutines* above), as intended, because it will run _before_ calling the solver.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randprob(n): \n",
    "    c = [[0 for x in xrange(n)] for x in xrange(n)] # initialise cost matrix\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                c[i][j] = 0 # zero diagonal\n",
    "            elif i > j:\n",
    "                c[i][j] = c[j][i] # symmetric matrix\n",
    "            else:\n",
    "                c[i][j] = random.expovariate(1) # more interesting than uniform weights\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate a problem instance and solve it.\n",
    "\n",
    "The extra `inst` argument in both the generators and the fitness function parallel the extra `problem_size` argument in `sphere.ipynb`. As in that case, if we want to use `solve`, we have to specialise these functions to a particular instance. So, we generate the instance and then use `functools.partial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  average trace size:  10\n",
      "[7, 9, 2, 3, 5, 4, 6, 8, 1, 0]\n",
      "-11.2286054988\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "inst = randprob(10) # generate a toy instance, represented by a city-city distance matrix\n",
    "randsol1_inst = lambda: randsol1(inst) # specialise the generator for that instance\n",
    "fitness_inst = lambda x: fitness(x, inst) # specialise the fitness for that instance\n",
    "\n",
    "ind, fit = solve(randsol1_inst, fitness_inst, solver=\"HC\", str_trace=True, effort=1.0)\n",
    "print(ind)\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments and Analysis\n",
    "---\n",
    "\n",
    "Next, we can do a large run and carry out some analysis on our results. \n",
    "We'll import functions provided by PTO, and also use scipy. `compare_all` is able to generate random problem instances internally, and specialise the fitness function and generator, so we pass in the original (not specialised) versions of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PTO import compare_all, stat_summary, make_table\n",
    "import scipy.stats # only for post-run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = compare_all(fitness, \n",
    "                      [randsol1, randsol2, randsol3], # multiple generators\n",
    "                      sizes=[20],\n",
    "                      random_instance=randprob, # new random instance for each run\n",
    "                      methods=[\"HC\"],\n",
    "                      str_traces=[False, True], # multiple trace types\n",
    "                      budget=10000, \n",
    "                      num_runs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1**: Compare structured trace with linear, using hill-climbing: no difference, using any generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean -4.64 std 0.60 min -5.91 med -4.40 max -3.62\n",
      "mean -5.02 std 0.96 min -7.87 med -4.74 max -3.97\n",
      "Ttest_indResult(statistic=1.2555906149948408, pvalue=0.21964173194211625)\n"
     ]
    }
   ],
   "source": [
    "d0, d1 = results[(20, False, 'randsol1', 'HC')], results[(20, True, 'randsol1', 'HC')]\n",
    "print(stat_summary(d0))\n",
    "print(stat_summary(d1))\n",
    "print(scipy.stats.ttest_ind(d0, d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean -6.43 std 1.19 min -8.79 med -6.24 max -4.71\n",
      "mean -6.87 std 1.14 min -9.47 med -6.65 max -4.96\n",
      "Ttest_indResult(statistic=1.0066463582924265, pvalue=0.32272534852071288)\n"
     ]
    }
   ],
   "source": [
    "d0, d1 = results[(20, False, 'randsol2', 'HC')], results[(20, True, 'randsol2', 'HC')]\n",
    "print(stat_summary(d0))\n",
    "print(stat_summary(d1))\n",
    "print(scipy.stats.ttest_ind(d0, d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean -2.72 std 0.50 min -4.02 med -2.63 max -1.55\n",
      "mean -2.68 std 0.43 min -3.59 med -2.67 max -1.71\n",
      "Ttest_indResult(statistic=-0.21221784629430332, pvalue=0.83347440749079049)\n"
     ]
    }
   ],
   "source": [
    "d0, d1 = results[(20, False, 'randsol3', 'HC')], results[(20, True, 'randsol3', 'HC')]\n",
    "print(stat_summary(d0))\n",
    "print(stat_summary(d1))\n",
    "print(scipy.stats.ttest_ind(d0, d1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 2**: Compare one generator against another, using structured trace. Some difference between randsol1 and 2, but large improvements with randsol3 (the one which uses the problem data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean -5.02 std 0.96 min -7.87 med -4.74 max -3.97\n",
      "mean -6.87 std 1.14 min -9.47 med -6.65 max -4.96\n",
      "Ttest_indResult(statistic=4.6664836419326408, pvalue=6.9005902412731272e-05)\n"
     ]
    }
   ],
   "source": [
    "d0, d1 = results[(20, True, 'randsol1', 'HC')], results[(20, True, 'randsol2', 'HC')]\n",
    "print(stat_summary(d0))\n",
    "print(stat_summary(d1))\n",
    "print(scipy.stats.ttest_ind(d0, d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean -5.02 std 0.96 min -7.87 med -4.74 max -3.97\n",
      "mean -2.68 std 0.43 min -3.59 med -2.67 max -1.71\n",
      "Ttest_indResult(statistic=-8.3136513164084, pvalue=4.7944607593335423e-09)\n"
     ]
    }
   ],
   "source": [
    "d0, d1 = results[(20, True, 'randsol1', 'HC')], results[(20, True, 'randsol3', 'HC')]\n",
    "print(stat_summary(d0))\n",
    "print(stat_summary(d1))\n",
    "print(scipy.stats.ttest_ind(d0, d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean -6.87 std 1.14 min -9.47 med -6.65 max -4.96\n",
      "mean -2.68 std 0.43 min -3.59 med -2.67 max -1.71\n",
      "Ttest_indResult(statistic=-12.92644502840748, pvalue=2.5267375309572285e-13)\n"
     ]
    }
   ],
   "source": [
    "d0, d1 = results[(20, True, 'randsol2', 'HC')], results[(20, True, 'randsol3', 'HC')]\n",
    "print(stat_summary(d0))\n",
    "print(stat_summary(d1))\n",
    "print(scipy.stats.ttest_ind(d0, d1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some real TSP instances: TSPLIB\n",
    "\n",
    "Up to now, we've been generating random TSP instances to demonstrate investigation of scalability. Next, we'll read some real TSP instances from TSPLIB.\n",
    "\n",
    "First we get TSPLIB itself. If the following code fails for any reason, just download `ALL_tsp.tar.gz` from the given URL, and extract it into a new directory `/TSPLIB` in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def get_TSPLIB(dirname):\n",
    "    import os\n",
    "    import tarfile\n",
    "    import requests # conda install requests, or pip install requests\n",
    "\n",
    "    tsplib_url = \"http://www.iwr.uni-heidelberg.de/groups/comopt/software/TSPLIB95/tsp/ALL_tsp.tar.gz\"\n",
    "    r = requests.get(tsplib_url)\n",
    "    os.makedirs(dirname)\n",
    "    filename = os.path.join(dirname, \"ALL_tsp.tar.gz\")\n",
    "    f = open(filename, \"wb\")\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "    tar = tarfile.open(filename, \"r:gz\")\n",
    "    tar.extractall(dirname)\n",
    "    tar.close()\n",
    "    \n",
    "if not os.path.isfile(os.path.join(dirname, \"att48.tsp.gz\")): # example to check if TSPLIB already exists\n",
    "    get_TSPLIB(\"TSPLIB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, gzip\n",
    "\n",
    "class TSP:\n",
    "    def __init__(self, filename):\n",
    "        self.cities = []\n",
    "        self.read_file(filename)\n",
    "        self.matrix = [[self.euclidean_distance(city_i, city_j) \n",
    "                        for city_i in self.cities] \n",
    "                       for city_j in self.cities]\n",
    "        # self.read_optimal_results(\"TSPLIB/STSP.html\")\n",
    "\n",
    "    def euclidean_distance(self, x, y):\n",
    "        return math.sqrt(sum((xi - yi) ** 2.0 for xi, yi in zip(x, y)))\n",
    "    \n",
    "    def read_optimal_results(self, filename):\n",
    "        # If we would like to look at known optima for these problem see\n",
    "        # http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html. Put\n",
    "        # that .html file under TSPLIB.\n",
    "        import re\n",
    "        optimal_results = {}\n",
    "        for line in open(filename).readlines():\n",
    "            p = r\">(\\w+) : (\\d+)<\"\n",
    "            m = re.search(p, line)\n",
    "            if m:\n",
    "                key, val = m.group(1, 2)\n",
    "                key = key.strip()\n",
    "                # optimal results are given as integers in TSPLIB\n",
    "                val = int(val.split()[0].strip())\n",
    "                optimal_results[key] = val\n",
    "        # print(\"Optimal results:\")\n",
    "        # print(optimal_results)\n",
    "        self.optimal = optimal_results[self.name]\n",
    "        print(\"Optimum: \" + str(self.optimal))\n",
    "\n",
    "    def read_file(self, filename):\n",
    "        \"\"\"FIXME this only works for files in the node xy-coordinate\n",
    "        format. Some files eg bayg29.tsp.gz give explicit edge weights\n",
    "        instead.\"\"\"\n",
    "        f = gzip.open(filename, \"rb\")\n",
    "        coord_section = False\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"NAME\"):\n",
    "                self.name = line.split(\":\")[1].strip()\n",
    "            elif (line.startswith(\"COMMENT\") or\n",
    "                  line.startswith(\"TYPE\") or\n",
    "                  line.startswith(\"EDGE_WEIGHT_TYPE\")):\n",
    "                pass\n",
    "            elif line.startswith(\"DIMENSION\"):\n",
    "                self.n = int(line.split(\":\")[1].strip())\n",
    "            elif line.startswith(\"NODE_COORD_SECTION\"):\n",
    "                coord_section = True\n",
    "            elif line.startswith(\"EOF\"):\n",
    "                break\n",
    "            elif coord_section:\n",
    "                # coords are sometimes given as floats in TSPLIB\n",
    "                idx, x, y = map(float, line.split(\" \"))\n",
    "                self.cities.append((x, y))\n",
    "\n",
    "        \n",
    "filenames = [\"att48.tsp.gz\", \"berlin52.tsp.gz\"]\n",
    "insts = [TSP(os.path.join(dirname, filename)).matrix for filename in filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `compare_all` with pre-made instances\n",
    "\n",
    "Now we have a set of problem instances on which we want to test. `compare_all` can handle that case too: we pass them in with `instances=[...]`, and omit the `sizes` and `random_instance` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_all(fitness, \n",
    "                      [randsol1, randsol3], # multiple generators\n",
    "                      instances=insts,\n",
    "                      methods=[\"HC\"],\n",
    "                      budget=10000, \n",
    "                      num_runs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for both of these problems, `randsol3` (using the GRASP-like problem data approach) wins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, True, 'randsol1', 'HC'): mean -70592.02 std 5223.95 min -78956.95 med -70283.02 max -59564.07\n",
      "(0, True, 'randsol3', 'HC'): mean -60951.05 std 4397.65 min -68988.38 med -60144.41 max -55396.89\n",
      "(1, True, 'randsol1', 'HC'): mean -15099.09 std 865.19 min -16477.81 med -15071.50 max -13831.87\n",
      "(1, True, 'randsol3', 'HC'): mean -13943.62 std 1144.72 min -15880.97 med -14135.62 max -12026.88\n"
     ]
    }
   ],
   "source": [
    "make_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "---\n",
    "\n",
    "We have demonstrated the use of a permutation search space, and the comparison of multiple generators, and the use of problem data in the generator for improved performance. And we have seen some more use of `compare_all` and statistical testing on its results, and how to read problem instances for TSP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
